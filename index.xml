<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Granular Material on Granular Material</title><generator uri="https://gohugo.io">Hugo</generator><link>https://dallascard.github.io/granular-material/</link><language>en-us</language><updated>Sun, 19 Feb 2023 13:04:13 -0500</updated><item><title>ChatGPT and Sociotechnical Instability</title><link>https://dallascard.github.io/granular-material/post/chatgpt_and_instability/</link><pubDate>Sun, 19 Feb 2023 13:04:13 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/chatgpt_and_instability/</guid><description>I&amp;rsquo;ve written about this before, but it&amp;rsquo;s worth remembering that almost nothing in sociotechnical systems is guaranteed to remain stable for very long. We&amp;rsquo;ve recently had two great examples of this, with the first being the changes to Twitter, and the second being ChatGPT (and, by extension, the new Bing).
In the first case, a platform which had long seemed relatively static, (especially compared to all the rest), rather suddenly changed hands, which led to major changes in what it delivered.</description></item><item><title>ChatGPT Dominance</title><link>https://dallascard.github.io/granular-material/post/chatgpt-dominance/</link><pubDate>Sat, 21 Jan 2023 14:39:35 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/chatgpt-dominance/</guid><description>I expect that almost anyone reading this will have heard of ChatGPT by now. Released about a month ago, ChatGPT is a system developed by OpenAI which provides text responses to text input. Although details are scarce, under the hood ChatGPT is basically a large language model, trained with some additional tricks (see Yoav Goldberg&amp;rsquo;s write up for a good summary). In other words, it is a model which maps from the text input (treated as a sequence of tokens), to a distribution over possible next tokens, and generates text by making repeated calls to this function, and sampling tokens from the predicted distributions.</description></item><item><title>AI, software, and governance</title><link>https://dallascard.github.io/granular-material/post/ai-software-governance/</link><pubDate>Mon, 12 Dec 2022 18:48:06 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/ai-software-governance/</guid><description>In a recent article covering the FTX collapse, the New York Times described large language models (LLMs) as &amp;ldquo;an increasingly powerful breed of A.I. that can write tweets, emails and blog posts and even generate computer programs.&amp;rdquo; There is a lot that we could pick apart in this definition (e.g., what makes LLMs part of a &amp;ldquo;breed&amp;rdquo;, what distinguishes the ability to write an email as opposed to a blog post, etc.), but for the moment I&amp;rsquo;d like to focus on the term &amp;ldquo;A.I.&amp;rdquo; (henceforth &amp;ldquo;AI&amp;rdquo;). Referring to LLMs as an example of AI is certainly not atypical. Indeed, it increasingly seems like LLMs have become one of the modern canonical examples of this concept. But why is it that we think of these systems as members of this category? And how much rhetorical work is being done by referring to LLMs as a type of &amp;ldquo;AI&amp;rdquo;, as opposed to &amp;ldquo;models&amp;rdquo;, &amp;ldquo;programs&amp;rdquo;, &amp;ldquo;systems&amp;rdquo;, or other similar categories?</description></item><item><title>Hacking LLM bots</title><link>https://dallascard.github.io/granular-material/post/remote-work-bot/</link><pubDate>Fri, 23 Sep 2022 10:39:28 -0400</pubDate><guid>https://dallascard.github.io/granular-material/post/remote-work-bot/</guid><description>&lt;p>For anyone who missed it, a Twitter account named @mkualquiera recently deployed what seems like a kind of adversarial attack in the wild on a large language model (LLM)-based Twitter bot. I&amp;rsquo;ll link to the key post below, but it&amp;rsquo;s worth providing a bit of context, as it wasn&amp;rsquo;t immediately clear to me what was going on when I first saw the tweet.&lt;/p>
&lt;figure>&lt;img src="./img/remote-work-bot/header.png"/>
&lt;/figure></description></item><item><title>About</title><link>https://dallascard.github.io/granular-material/about/</link><pubDate>Mon, 01 Aug 2022 16:42:18 +0400</pubDate><guid>https://dallascard.github.io/granular-material/about/</guid><description>This blog is about exploring topics related to science, technology, and culture in slightly more detail than is reasonable. Written by Dallas Card</description></item><item><title>Report from FAccT 2022</title><link>https://dallascard.github.io/granular-material/post/facct-2022/</link><pubDate>Sun, 03 Jul 2022 12:59:04 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/facct-2022/</guid><description>The fifth iteration of FAccT (the ACM Conference on Fairness, Accountability, and Transparency) was held earlier this month (June 21–24) in Seoul, South Korea. More than just a hybrid conference, this was actually a full in-person conference, combined with a full on-line conference. These happened in parallel, with virtual sessions starting before and continuing after the in-person component each day. Around 500 people attended in person, with another 500 participating remotely.</description></item><item><title>Counting Deaths</title><link>https://dallascard.github.io/granular-material/post/counting-deaths/</link><pubDate>Sat, 14 May 2022 17:24:13 -0400</pubDate><guid>https://dallascard.github.io/granular-material/post/counting-deaths/</guid><description>Although morbid, it&amp;rsquo;s fascinating to read a recent article in the NYT about efforts in Sierra Leone to use &amp;ldquo;electronic autopsies&amp;rdquo; in a large scale attempt at counting deaths. According to the article, this undertaking is part of a broader effort at data collection, including questions on age, religion, marital status, etc. The novelty, it seems, is in trying to be thorough with respect to what people have died of (including extensive questions about symptoms), even though this information is being collected potentially long after the fact.</description></item><item><title>Modular Domain Adaptation</title><link>https://dallascard.github.io/granular-material/post/modular/</link><pubDate>Mon, 25 Apr 2022 20:16:54 -0400</pubDate><guid>https://dallascard.github.io/granular-material/post/modular/</guid><description>Despite their limitations, off-the-shelf models are still quite widely used by computational social science researchers for measuring various properties of text, including both lexicons, like LIWC, and cloud-based APIs, like Perspective API. The approach of using an off-the-shelf model has some definite advantages, including standardization and reproducibility, but such models may not be reliable when applied to a domain that differs from the ones on which they were developed&amp;hellip;</description></item><item><title>Stability and Change</title><link>https://dallascard.github.io/granular-material/post/stability/</link><pubDate>Sat, 12 Mar 2022 17:40:34 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/stability/</guid><description>One of the biggest frustrations with software is that things are constantly changing. From operating systems to apps to web interfaces, things rarely remain the same for very long, especially for users of Windows or MacOS.
There are many reasons for this of course. For decades, hardware has continued to improve at a steady rate, and so software is constantly being rewritten to take advantage of the latest capabilities. Moreover, the incredibly sloppy standard for software quality and reliability (compared to traditional engineering disciplines) means that even the most professional software is shipped with massive numbers of bugs and vulnerabilities, which constantly need to be patched.</description></item><item><title>Confidence in Science</title><link>https://dallascard.github.io/granular-material/post/confidence-in-science/</link><pubDate>Sun, 06 Mar 2022 09:20:14 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/confidence-in-science/</guid><description>I’ve been thinking recently about the role of confidence in science, and how long beliefs can persist simply because everyone else seems to believe them. Coincidentally, Andrew Gelman posted about this two days ago, responding to comments from a biologist about how the replication crisis had not been a major problem in biology. Her argument was that this was because biology is a &amp;ldquo;cumulative science&amp;rdquo;. By this she meant that when something important gets published, it is often the kind of discovery that people want to use immediately.</description></item><item><title>AI Dermatology: Part 2</title><link>https://dallascard.github.io/granular-material/post/ai-dermatology-2/</link><pubDate>Sat, 19 Feb 2022 16:26:43 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/ai-dermatology-2/</guid><description>​In the last post, I discussed the possible broader implications of Google&amp;rsquo;s recent foray into making an AI dermatology tool. In this follow up post, I want to focus on the research behind the product announcement, bringing a slightly critical eye.</description></item><item><title>AI Dermatology: Part 1</title><link>https://dallascard.github.io/granular-material/post/ai-dermatology-1/</link><pubDate>Sat, 05 Feb 2022 07:32:51 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/ai-dermatology-1/</guid><description>Midway through last year Google announced a new foray into the medical technology space, sharing that it was developing an &amp;ldquo;AI-powered dermatology assist tool&amp;rdquo;&amp;mdash;a phone-based app that would allow users to take photos of skin lesions and retrieve information about relevant medical conditions from the web. Similar apps already exist, but it&amp;rsquo;s fair to say that a comparable effort by Google is likely to have much more significant effects on how people interact with the medical system, their personal data, and even their own bodies.</description></item><item><title>Vaccine Allocation at Stanford Hospital</title><link>https://dallascard.github.io/granular-material/post/stanford-vaccines/</link><pubDate>Sat, 19 Dec 2020 21:43:54 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/stanford-vaccines/</guid><description>In a video that was widely shared last Friday, a representative from Stanford Medical Center spoke to residents protesting how the hospital chose to allocate its first shipment of COVID-19 vaccines. The hospital had around 5,000 initial doses to distribute (and expects to have tens of thousands more within the next few weeks), and came up with an allocation scheme in which only 7 of the approximately 1,300 residents were on the list. Many of these residents deal directly with patients who have COVID-19, whereas other more senior physicians, as well as other front line workers, such as nurses and food service employees, were given priority. In the video, the spokesperson explains that the algorithm they used to come up with an allocation scheme “clearly didn’t work”, to which protestors respond by shouting “Algorithms suck!” and “Fuck the algorithm!” &amp;hellip;</description></item><item><title>The case for professional critics in science</title><link>https://dallascard.github.io/granular-material/post/science-critics/</link><pubDate>Sat, 03 Oct 2020 21:24:17 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/science-critics/</guid><description>In many areas of science, there is an increasingly urgent unmet need, a role that could be simultaneously fascinating, rewarding, and potential remunerative. It is a role that already exists in various forms, but which could be made into something much more potent, especially if forces converged to make it more prominent. I am talking, of course, about the professional science critic.
In the popular imagination, science operates something like a priesthood: scientists enter elite institutions as novices and emerge years later as full-fledged representatives of The Truth.</description></item><item><title>Representational Power</title><link>https://dallascard.github.io/granular-material/post/representational-power/</link><pubDate>Sat, 20 Jul 2019 21:27:37 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/representational-power/</guid><description>&lt;figure>&lt;a href="https://en.wikipedia.org/wiki/View_from_the_Window_at_Le_Gras">&lt;img src="./img/representational-power//plate.jpeg"
alt="The original plate of “View from the Window at Le Gras”, a heliograph made by Nicéphore Niépce around 1827."/>&lt;/a>&lt;figcaption>
&lt;p>The original plate of “View from the Window at Le Gras”, a heliograph made by Nicéphore Niépce around 1827.&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>Although it isn’t normally thought of in these terms, taking a photograph involves recording a four-dimensional block of space-time and projecting it down to a two-dimensional representation. With sufficiently sensitive material (and a fast enough shutter), one can produce images more or less instantaneously, but longer exposures reveal the inherent temporality of this process, showing us something that is clearly based on the world, yet quite different from our experience of it. Today, the ability to create images is so commonplace, of course, that we easily take it for granted, but early commentaries on photography reveal just how extraordinary it once was. Indeed, the history of photography provides both a compelling example of the power of representation, and a useful parallel to more recent forms of technological magic, especially that of machine learning.&lt;/p></description></item></channel></rss>