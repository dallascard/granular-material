<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Digital-Preservation on Granular Material</title><link>https://dallascard.github.io/granular-material/tags/digital-preservation/</link><description>Recent content in Digital-Preservation on Granular Material</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 31 Mar 2024 20:08:01 -0400</lastBuildDate><atom:link href="https://dallascard.github.io/granular-material/tags/digital-preservation/index.xml" rel="self" type="application/rss+xml"/><item><title>Financing Common Crawl</title><link>https://dallascard.github.io/granular-material/post/financing-common-crawl/</link><pubDate>Sun, 31 Mar 2024 20:08:01 -0400</pubDate><guid>https://dallascard.github.io/granular-material/post/financing-common-crawl/</guid><description>Mozilla recently published an excellent new report out about Common Crawl, the non-profit whose web crawls have played an important role in the development of numerous large language models (LLMs). Written by Stefan Baack and Mozilla Insights, the report is based on both public documents and new interviews with Common Crawl&amp;rsquo;s current director and crawl engineer, and goes into some detail about the history of the organization, and how its data is being used.</description></item><item><title>Ubi Sunt</title><link>https://dallascard.github.io/granular-material/post/ubi_sunt/</link><pubDate>Sun, 17 Sep 2023 13:05:18 -0400</pubDate><guid>https://dallascard.github.io/granular-material/post/ubi_sunt/</guid><description>&lt;p>There is a duality at the heart of large language models. On the one hand, they are essentially a backwards-looking invention, a &lt;a href="https://www.youtube.com/live/k7rPtFLH6yw?si=bD6ir6cwtpOtEV0i">&amp;ldquo;cultural technology&amp;rdquo;&lt;/a>, in the words of Alison Gopnik&amp;mdash;algorithms which index and remix a large slice of human culture (though one that is typically heavily biased towards the recent past). On the other hand, they can often seem to be producing something entirely new, and can thereby leave many people with the impression of having a personality or even &amp;ldquo;sentience&amp;rdquo; (whatever that means exactly); in the most extreme cases, &lt;a href="https://youtu.be/NgHFMolXs3U?si=gfKBPeOlEhQeDdEg">some people&lt;/a> have apparently convinced themselves that such models are a step on the path towards some sort of successor species to humanity, a new regime of algorithmic children that will survive our own human catastrophes. Complicating matters here is the fact that the emergence of and widespread attention to these systems largely overlapped with the Covid-19 pandemic, a time in which we have all had additional reason to reflect on life, death, loss, and creation.&lt;/p></description></item><item><title>Reproducibility in Art and Science</title><link>https://dallascard.github.io/granular-material/post/reproducibility-in-art-and-science/</link><pubDate>Mon, 05 Jun 2017 20:43:20 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/reproducibility-in-art-and-science/</guid><description>&lt;p>Marking the beginning of a new &lt;a href="http://rhizome.org/editorial/2017/may/30/rhizome-google-partnership/">partnership&lt;/a> between Rhizome and the Google Cultural Institute, the Rhizome blog recently published a &lt;a href="http://rhizome.org/editorial/2017/may/30/preservation-by-accident-is-not-a-plan/">conversation&lt;/a> between Dragan Espenschied, Rhizome’s preservation director, and Vint Cerf, Google’s Chief Internet Evangelist. In addition to bringing together two people with among the coolest job titles ever, it got me thinking about some similarities between art and science when it comes to the issue of reproduction.&lt;/p>
&lt;p>The main thrust of the conversation is about the difficulty of preserving internet art. In particular, works such as &lt;a href="https://anthology.rhizome.org/the-web-stalker">The Web Stalker&lt;/a> were often made in a particular context, intended to be performances of a sort, and which depended on the existence of a certain technical infrastructure, including a particular operating system, a particular input device, an internet connection, and accompanying protocols. Part of the conversation relates to the fact that it’s actually very difficult to maintain or recreate every single thing that is required in order for a modern viewer to have the exact same experience as when it was first created. Nevertheless, Dragan argues, there is still value in preserving part of the experience.&lt;/p></description></item></channel></rss>