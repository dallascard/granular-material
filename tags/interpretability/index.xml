<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Interpretability on Granular Material</title><link>https://dallascard.github.io/granular-material/tags/interpretability/</link><description>Recent content in Interpretability on Granular Material</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 25 May 2018 20:46:57 -0500</lastBuildDate><atom:link href="https://dallascard.github.io/granular-material/tags/interpretability/index.xml" rel="self" type="application/rss+xml"/><item><title>What everyone needs to know about interpretability in machine learning</title><link>https://dallascard.github.io/granular-material/post/interpretability/</link><pubDate>Fri, 25 May 2018 20:46:57 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/interpretability/</guid><description>For anyone who’s been paying attention, it should be apparent that statistical machine learning systems are being widely deployed for automated decision making in all kinds of areas these days, including criminal justice, medicine, education, employment, policing, and so on. Particularly with the recently enacted GDPR&amp;mdash;the new European regulation about data and privacy&amp;mdash;there is growing interest in having systems that are interpretable, that is, we can make some sense of why they are making the prediction that they are making. To borrow an example from Been Kim, if a computer tells you that you need surgery, you’re probably going to ask for some sort of explanation.</description></item></channel></rss>