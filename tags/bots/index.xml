<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>bots on Granular Material</title><link>https://dallascard.github.io/granular-material/tags/bots/</link><description>Recent content in bots on Granular Material</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 23 Sep 2022 10:39:28 -0400</lastBuildDate><atom:link href="https://dallascard.github.io/granular-material/tags/bots/index.xml" rel="self" type="application/rss+xml"/><item><title>Hacking LLM bots</title><link>https://dallascard.github.io/granular-material/post/remote-work-bot/</link><pubDate>Fri, 23 Sep 2022 10:39:28 -0400</pubDate><guid>https://dallascard.github.io/granular-material/post/remote-work-bot/</guid><description>For anyone who missed it, a Twitter account named @mkualquiera recently deployed what seems like a kind of adversarial attack in the wild on a large language model (LLM)-based Twitter bot. I&amp;rsquo;ll link to the key post below, but it&amp;rsquo;s worth providing a bit of context, as it wasn&amp;rsquo;t immediately clear to me what was going on when I first saw the tweet.
The bot itself (@remoteli_io) claims it &amp;ldquo;helps you discover remote jobs which allow you to work from anywhere&amp;rdquo;.</description></item></channel></rss>