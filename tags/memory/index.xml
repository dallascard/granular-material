<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Memory on Granular Material</title><link>https://dallascard.github.io/granular-material/tags/memory/</link><description>Recent content in Memory on Granular Material</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 25 Oct 2023 11:05:12 -0400</lastBuildDate><atom:link href="https://dallascard.github.io/granular-material/tags/memory/index.xml" rel="self" type="application/rss+xml"/><item><title>Refik Anadol</title><link>https://dallascard.github.io/granular-material/post/refik-anadol/</link><pubDate>Wed, 25 Oct 2023 11:05:12 -0400</pubDate><guid>https://dallascard.github.io/granular-material/post/refik-anadol/</guid><description>Last week&amp;rsquo;s speaker in the Penny Stamps Speaker Series was Refik Anadol, one of the most successful artists working primarily with AI, data, and visualization. Over a decade or so, Anadol has produced work in collaboration with organizations and venues like the LA Philharmonic, Gaudi&amp;rsquo;s Casa Batlló, the Exosphere in Las Vegas, and the Museum of Modern Art in New York.
The lecture he gave was basically a review of his body of work, and that of his art and design studio, which is comprised of about 20 people.</description></item><item><title>Ubi Sunt</title><link>https://dallascard.github.io/granular-material/post/ubi_sunt/</link><pubDate>Sun, 17 Sep 2023 13:05:18 -0400</pubDate><guid>https://dallascard.github.io/granular-material/post/ubi_sunt/</guid><description>There is a duality at the heart of large language models. On the one hand, they are essentially a backwards-looking invention, a &amp;ldquo;cultural technology&amp;rdquo;, in the words of Alison Gopnik&amp;mdash;algorithms which index and remix a large slice of human culture (though one that is typically heavily biased towards the recent past). On the other hand, they can often seem to be producing something entirely new, and can thereby leave many people with the impression of having a personality or even &amp;ldquo;sentience&amp;rdquo; (whatever that means exactly); in the most extreme cases, some people have apparently convinced themselves that such models are a step on the path towards some sort of successor species to humanity, a new regime of algorithmic children that will survive our own human catastrophes.</description></item><item><title>Emergent Archives</title><link>https://dallascard.github.io/granular-material/post/emergent-archives/</link><pubDate>Mon, 27 Nov 2017 22:12:15 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/emergent-archives/</guid><description>&lt;figure>&lt;img src="../../img/emergent-archives/archives.jpg">
&lt;/figure>

&lt;p>It’s fascinating to think about how much we give to the internet, and sometimes, how much it gives back. In my mind, there is a fairly direct historical connection between self-archiving practices from a couple of decades ago (keeping a diary, making photo albums, writing letters, etc.), to all the more recent variations on this idea (status updates, tweets, photo feeds, etc.). The difference is arguably in how much of the background work is taken care of for us, and the fact that we now make so much of this information public or semi-public. While many have explored the implications of this for privacy, security, and surveillance, there is another aspect that gets less attention: the automatic creation of our own archives.&lt;/p></description></item></channel></rss>