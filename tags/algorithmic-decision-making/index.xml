<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>algorithmic-decision-making on Granular Material</title><link>https://dallascard.github.io/granular-material/tags/algorithmic-decision-making/</link><description>Recent content in algorithmic-decision-making on Granular Material</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 12 Dec 2022 18:48:06 -0500</lastBuildDate><atom:link href="https://dallascard.github.io/granular-material/tags/algorithmic-decision-making/index.xml" rel="self" type="application/rss+xml"/><item><title>AI, software, and governance</title><link>https://dallascard.github.io/granular-material/post/ai-software-governance/</link><pubDate>Mon, 12 Dec 2022 18:48:06 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/ai-software-governance/</guid><description>In a recent article covering the FTX collapse, the New York Times described large language models (LLMs) as &amp;ldquo;an increasingly powerful breed of A.I. that can write tweets, emails and blog posts and even generate computer programs.&amp;rdquo; There is a lot that we could pick apart in this definition (e.g., what makes LLMs part of a &amp;ldquo;breed&amp;rdquo;, what distinguishes the ability to write an email as opposed to a blog post, etc.), but for the moment I&amp;rsquo;d like to focus on the term &amp;ldquo;A.I.&amp;rdquo; (henceforth &amp;ldquo;AI&amp;rdquo;). Referring to LLMs as an example of AI is certainly not atypical. Indeed, it increasingly seems like LLMs have become one of the modern canonical examples of this concept. But why is it that we think of these systems as members of this category? And how much rhetorical work is being done by referring to LLMs as a type of &amp;ldquo;AI&amp;rdquo;, as opposed to &amp;ldquo;models&amp;rdquo;, &amp;ldquo;programs&amp;rdquo;, &amp;ldquo;systems&amp;rdquo;, or other similar categories?</description></item><item><title>Report from FAccT 2022</title><link>https://dallascard.github.io/granular-material/post/facct-2022/</link><pubDate>Sun, 03 Jul 2022 12:59:04 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/facct-2022/</guid><description>The fifth iteration of FAccT (the ACM Conference on Fairness, Accountability, and Transparency) was held earlier this month (June 21–24) in Seoul, South Korea. More than just a hybrid conference, this was actually a full in-person conference, combined with a full on-line conference. These happened in parallel, with virtual sessions starting before and continuing after the in-person component each day. Around 500 people attended in person, with another 500 participating remotely.</description></item><item><title>Vaccine Allocation at Stanford Hospital</title><link>https://dallascard.github.io/granular-material/post/stanford-vaccines/</link><pubDate>Sat, 19 Dec 2020 21:43:54 -0500</pubDate><guid>https://dallascard.github.io/granular-material/post/stanford-vaccines/</guid><description>In a video that was widely shared last Friday, a representative from Stanford Medical Center spoke to residents protesting how the hospital chose to allocate its first shipment of COVID-19 vaccines. The hospital had around 5,000 initial doses to distribute (and expects to have tens of thousands more within the next few weeks), and came up with an allocation scheme in which only 7 of the approximately 1,300 residents were on the list. Many of these residents deal directly with patients who have COVID-19, whereas other more senior physicians, as well as other front line workers, such as nurses and food service employees, were given priority. In the video, the spokesperson explains that the algorithm they used to come up with an allocation scheme “clearly didn’t work”, to which protestors respond by shouting “Algorithms suck!” and “Fuck the algorithm!” &amp;hellip;</description></item></channel></rss>