<!doctype html><html>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<title>What everyone needs to know about interpretability in machine learning - Granular Material</title>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content>
<meta name=author content>
<meta name=keywords content>
<link rel=canonical href=https://dallascard.github.io/granular-material/post/interpretability/>
<link rel=stylesheet type=text/css href=https://dallascard.github.io/granular-material//css/combined-min.css>
</head>
<body>
<div class=site-wrap>
<header class="site-header px2 px-responsive">
<div class="mt2 wrap">
<div class=measure>
<a href=https://dallascard.github.io/granular-material/ class=site-title>Granular Material</a>
<nav class="site-nav right">
<a href=https://dallascard.github.io/granular-material//tags/>Tags</a>
</form>
</nav>
<div class=clearfix></div>
</div>
</div>
</header>
<div class="post p2 p-responsive wrap" role=main>
<div class=measure>
<div class="post-header mb2">
<h1 class=py2>What everyone needs to know about interpretability in machine learning</h1>
<span class=post-meta>May 25, 2018 </span><br>
</div>
<article class=post-content>
<p><em>Note: this post was written for a general audience, and assumes only passing familiarity with machine learning.</em></p>
<p>For anyone who’s been paying attention, it should be apparent that statistical machine learning systems are being widely deployed for automated decision making in all kinds of areas these days, including criminal justice, medicine, education, employment, policing, and so on. (For a great overview of the hazards of applying machine learning in these domains, check out Cathy O’Neil’s book <a href=https://weaponsofmathdestructionbook.com/><em>Weapons of Math Destruction</em></a>).</p>
<p>Particularly with the recently enacted <a href=https://www.eugdpr.org/>GDPR</a>&mdash;the new European regulation about data and privacy&mdash;there is growing interest in having systems that are <em>interpretable</em>, that is, we can make some sense of why they are making the prediction that they are making. To borrow an example from <a href=https://people.csail.mit.edu/beenkim/>Been Kim</a>, if a computer tells you that you need surgery, you’re probably going to ask for some sort of explanation.</p>
<p>There is now essentially a whole <a href=https://www.fatml.org/>subfield</a> of research devoted to interpretability in machine learning, so there’s no chance of covering all of that here. However, given how much confusion seems to be taking place, I thought it would be useful to outline a few essential ideas that everyone should know about this area.</p>
<p><strong>1. Machine learning systems make predictions based on a set of input features (i.e. a bunch of numbers).</strong></p>
<p>This point is in some sense so obvious that it’s rarely discussed, but it is actually quite important. Although machine learning is being used for all kinds of data, including images, videos, and natural language, the first step of any such system is to convert the input into a set of numbers. In the case of a medical image, for example, the image would be converted into a set of numbers representing pixel intensities, one for each pixel.</p>
<p>For images, this will be a very high fidelity representation, as this is in some sense how the data is natively acquired. For other domains, however, the mapping may involve some loss of information. For example, in working with text data, the text is typically processed in a way that involves some sort of simplification, such as dropping rare words. The words themselves would be converted to numbers, perhaps by using an index which maps each unique word type to a unique numeric representation (or other more complex variations on this).</p>
<p>At this point you are probably thinking “but, I am large! I contain multitudes! I cannot be represented by a set of numbers!” and in many ways you would be correct. For example, the much-discussed <a href=https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing>ProPublica story</a> on the use of risk assessments in criminal justice focused on a particular system that took as input the answers to 137 questions. Although this still produces a vast space of possible inputs, it seems trivially obvious that these questions will fail to capture all the nuance of each individual. Any individuals who provide the exact same set of answers would be treated as functionally identical, even though there would almost certainly be differences between them that were not captured by the questions.</p>
<p>In some cases, these details may not matter all that much. In medical diagnosis, for example, a doctor may not need to know everything about you in order to diagnose your illness; perhaps a simple echo-cardiogram will do. In other areas it is much less clear how much information is needed. Credit scoring is almost certainly effective to some degree, given the financial incentives involved, but making predictions is always hard (yes, especially about the future), and it is difficult to know whether there is additional information out there that could have been helpful.</p>
<p>Although modern machine learning systems are often described as being good at representation learning, meaning that less work is required in terms of hand-crafting features (intelligent ways of combining the inputs), they still operate on a set of numbers, and anything which is not included in the input data will never be considered. Although we have many ways of determining if a particular feature can safely be ignored, we are not particularly good at knowing whether we are missing some critical piece of information.</p>
<p>There is much more to be explored here, particularly the question of who has the power to define how people will be represented, but the main thing to keep in mind is that representing inputs to a machine learning system (or “model” or “algorithm”) always involves choices. If you’re thinking about interpretability, the first question to ask is what data does the system take as input, how was this decided, and what might be missing?</p>
<p><strong>2. Machine learning discovers correlations in data (but typically does not understand causality).</strong></p>
<p>This point is a bit more subtle than the first one, but the key idea is embedded in the old phrase that “correlation does not imply causation.” Although there is some <a href=https://www.theatlantic.com/technology/archive/2018/05/machine-learning-is-stuck-on-asking-why/560675/>exciting work</a> happening in causal machine learning these days, the vast majority of applied systems completely ignore it, and only focus on correlations.</p>
<p>What does this mean? Basically if I see a pattern in the input data that always (or frequently) occurs with a particular outcome (e.g. cancerous or benign), then I will quite sensibly predict that output when I see that input pattern. The whole idea of supervised learning is automatically discovering patterns in large amounts of data that allow us to map from a given input to a predicted label or outcome. This does not, however, imply that there is any direct causal connection between the outcomes and the patterns that have been found.</p>
<p>For example, think of the famous “<a href=https://en.wikipedia.org/wiki/Stanford_marshmallow_experiment>marshmallow experiment</a>”. Give a child a marshmallow and tell them that if they don’t eat it right away, they can have two later. In addition to the supposedly delightful videos of children trying to fight their instincts, this experiment produced the finding that those who are able to delay their gratification will have better life outcomes in various ways (on average). Although one interpretation is that eating the marshmallow somehow made things worse for those who could not resist doing so, a much more reasonable interpretation is that there is some latent property which is being measured, such as self-control, and that this explains both the marshmallow eating and the later life events.</p>
<p>In the same way, machine learning might learn a correlation between a certain pattern of behaviour (such as cycling to work) and later outcomes (such as <a href=https://www.forbes.com/sites/kevinmurnane/2017/04/25/new-research-indicates-cycling-to-work-has-extraordinary-health-benefits/#37acef03e62d>not dying</a>). This might then be used to predict future behaviour, but this should <strong>not</strong> be interpreted as one thing causing another. Rather, the system is as robust as the correlation (which could be the result of a causal effect).</p>
<p>Because machine learning systems work by discovering patterns in the data they were trained on, they tend not to be robust to changes in the distribution of input data. When a system has been trained on one dataset (such as <a href=https://www.theatlantic.com/daily-dish/archive/2010/10/western-educated-industrialized-rich-and-democratic/181667/>WEIRD</a> people), and is then applied to another, things can be expected to fail arbitrarily badly. This phenomena shows up in things like the <a href=https://www.wired.com/2015/10/can-learn-epic-failure-google-flu-trends/>failure</a> of Google’s flu trends, or <a href=https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html>computer vision systems</a> that <a href=https://www.wired.com/2009/12/hp-notebooks-racist/>only work</a> for some skin tones. Where things get really complicated is when a system itself starts <a href=https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist>interacting</a> with the environment, such that people start modifying their own behaviour in response to a system’s output. In such a case, all bets are truly off.</p>
<p><strong>3. Some models are special, but interpretability is not the norm.</strong></p>
<p>Given the above two points, how does supervised learning work? Well, the details of how we obtain a system can be complicated, but the end result is almost always the same. We start with some dataset used for training, and we end up with a system that takes a set of numbers as input and returns a different number as a prediction, based on correlations discovered in the training data.</p>
<p>For example, consider the question of how much money a house will sell for. We can dream up any number of numbers we might want to use to describe a house, such as number of bedrooms, square footage, the average price of a house in the neighbourhood, and so on, and learn a model that will map from these numbers to a dollar amount. A natural way to approach this is to get a large <a href=https://www.kaggle.com/c/house-prices-advanced-regression-techniques>dataset</a> of houses that have sold in the past, including how much they sold for, and then to train a model on this data to predict sale price from these features.</p>
<p>If we use a very simple model, we might well find some nice simple patterns, such as larger houses sell for more money. But perhaps this is only true up to a point; or perhaps it depends on the age of the house, because older houses tend to be bigger. As we start to consider more and more complicated possibilities, we will get a richer model, but also one that is harder for us to make sense of in any intuitive way. In the extreme, we basically end up with something that might as well be a black box, in that we simply input all the feature values, and get the predicted price in return. Note that it is <a href=https://towardsdatascience.com/the-black-box-metaphor-in-machine-learning-4e57a3a1d2b0>not <em>actually</em> a black box</a>, in that we can directly inspect the computations involved, but for complicated models, this may not be very meaningful. The main point is that in many ways the defining property of such systems is an explicit mapping from any possible point in the input space to a predicted output.</p>
<p>Now, it turns out that mappings allowed by certain classes of models can be summarized in a way which can easily be understood, interpreted, and simulated by humans. The classic example here is linear models, where the output is a weighted sum of the input values. For such a model, we can summarize how the output will change in response to a change in one input feature using a single number (the weight on that feature). This means that it is much easier for humans to feel like they understand how the model is making predictions.</p>
<p>An even simpler case is a decision list, which takes the form of a series of yes/no question. Although such a model has a simple interpretation that is easy for us as humans to process and describe in words, it is the same as any other model in that it takes a set of numbers (a set of 1s and 0s for yes and no), and returns another number as a prediction. For any machine learning system, once we have defined the input space (e.g., the set of questions), we can give it any possible set of numbers in that space, and it will return a prediction, even if it hasn’t seen that input pattern before. In some sense, the “magic” of machine learning is the ability to generalize from patterns that have been discovered to new combinations of inputs that have not previously been seen. However, this will always be a somewhat limited ability, potentially fragile, and dependent on various assumptions.</p>
<p>Models like linear models and decision lists are special, in that this mapping from inputs to outputs can be described in a compact way that humans can easily mentally simulate, but we should not necessarily expect this to be the norm. Especially when dealing with deep learning models and very large input spaces (such as images or language), the whole explanation for why a model made a particular prediction is that you gave it a particular set of inputs, and it performed a series of linear algebra operations that produced the results. Give it a different set of inputs and you will get a different prediction.</p>
<p>Although there is <a href=https://arxiv.org/pdf/1602.04938.pdf>lots</a> <a href=https://arxiv.org/pdf/1705.07874.pdf>of</a> <a href=https://arxiv.org/pdf/1703.04730.pdf>work</a> on how we might be able to provide easy-to-understand explanations which <em>approximate</em> the truth, this necessarily entails some degree of inaccuracy. Realistically, once a system has been trained, there should be no expectation that it can necessarily be simplified to something that has a compact explanation, at least not without some loss of fidelity to what the system is actually doing.</p>
<p>Is the situation hopeless then? Why no! That brings me to my last point:</p>
<p><strong>4. The real question is: how was the system created?</strong></p>
<p>Given that the mapping of a machine learning system from inputs to outputs seems so potentially arbitrary, one should rightfully ask, why is the model configured to make those specific predictions for those specific inputs? That is an excellent question, and the answer is all in how it was trained.</p>
<p>In particular, there are three essential elements that can be assayed. First, there is the question of what data was used to train the model. As we know, machine learning will discover patterns in the data it was trained on, but what data was originally used for this purpose? If it was a dataset of images of faces, whose faces? If it was credit histories, who was in the database? This has important implications for a potential mismatch between the training data used and where it is being deployed, as described above, but it also raises the question of whether there may be particular <a href="https://www.youtube.com/watch?v=fMym_BKWQzk">biases</a> baked into the dataset which unfairly reflect past inequalities.</p>
<p>Second, there is the question of how the data was represented (see the first point above). In some cases this will go hand in hand with the question of what training data was used, as lots of datasets have effectively been pre-coded into a particular representation. Nevertheless, we should not assume that this is the full picture, or the only way that such data <em>could</em> have been represented.</p>
<p>Finally, there is the question of what type of model was chosen and how it was trained. There is a huge range of possibilities here, but the details need not concern us. The point is not necessarily to ask whether the “right” choice was made here, but rather to acknowledge and investigate what else might have been possible. For certain types of models, we are guaranteed to get the same result for the combination of a choice of model and a particular dataset with a particular representation. For deep learning, by contrast, there is some inherent randomness involved, such there is always the possibility of ending up with a different set of model weights, even due to factors such as how parameters are initialized.</p>
<p>Of course, given a sufficiently large dataset, these nitty-gritty details about training are unlikely to make much difference. But, as data grows increasingly high-dimensional (think of all the different attributes of people that might be relevant to any particular prediction; in the extreme, think of the <em>billions</em> of base pairs in our DNA!), what might have seemed like lots of data, can suddenly feel quite sparse in the tails of the distribution. Particularly for unusual cases, it is highly possible that different choices about dataset, representation, and modeling would have resulted in a model that would have made a vastly different prediction.</p>
<p>To some extent choices can be partially justified (typically because one model worked better than another on some held-out data), but this is not the same as being able to claim that one has discovered the “true” model in any meaningful sense. Nor does accuracy necessarily tell the full story. Even models that obtain relatively high accuracy on held out data can still be <a href=https://arxiv.org/abs/1707.09457>biased in problematic ways</a>.</p>
<p>Unfortunately, it may not always be possible to provide a complete understanding, because such an understanding is partially a question about why we ended up with a particular system, and that comes down to the modeling choices that were made and the training data that was used. Particularly in cases where the data is sensitive or private, it does not seem feasible to allow everyone to inspect everyone else’s data in order to understand their own prediction. This seems especially true, given that the whole idea behind the GDPR was to give people more control over their data!</p>
<p>There are no easy answers here, but there have been many interesting proposals, with more coming all the time. For example, two recent papers&mdash;<a href=https://arxiv.org/pdf/1803.09010.pdf>datasheets for datasets</a> and <a href=https://medium.com/@AINowInstitute/algorithmic-impact-assessments-toward-accountable-automation-in-public-agencies-bd9856e6fdde>algorithmic impact assessments</a>&mdash;suggest ways in which we can attempt to protect ourselves against unintended biases or misuse of data.</p>
<p>Ultimately, transparency of some kind seems critical, but exactly what form that will take is still far from certain. The technology defines what is possible, but it comes down to regulation like the GDPR to determine what is permitted and what is required. Now, if only it were easy to <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3039125">interpret the regulation</a>&mldr;</p>
<p><strong>Key takeaways:</strong></p>
<ul>
<li>Some models are easy for humans to interpret, but this is the exception more than the rule; in general, we should not assume that all models can be represented in a way that is easy for humans to understand, at least not without some loss of fidelity.</li>
<li>In many ways the more important question than how a model works, is why we ended up with that particular model; ultimately, this will always be the result of the training data that was used, how that data was represented, and the modeling decisions that were</li>
<li>When applying machine learning in social domain, it is especially important to think about the training data being used, and to ask if it may be limited or biased in some way; it is much easier to rule out some feature as irrelevant than it is to know if some critical feature may be missing.</li>
<li>Finally, remember that the vast majority of supervised machine learning models work by discovering correlations in the data; without further evidence, this should not be interpreted as imply any kind of causal connection between inputs and outputs.</li>
</ul>
</article>
<div style=text-align:center>
<span class=post-meta> [<a href=https://dallascard.medium.com/what-everyone-needs-to-know-about-interpretability-in-machine-learning-d5ce16730407 target=_blank>Comments on Medium</a>] </span>
</div>
<br>
<p class=post-meta>Tags:&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/machine-learning/>machine-learning</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/interpretability/>interpretability</a>
</p>
</div>
</div>
</div>
<footer class=footer>
<div class="p2 wrap">
<div class="measure mt1 center">
<nav class="social-icons icons">
<a class="fa fa-rss rss" href=../../index.xml></a>
<a class="fa fa-twitter twitter" href=https://twitter.com/dallascard></a>
</nav>
<small>
Copyright &#169; 2017<br>
Powered by <a href=http://gohugo.io/ target=_blank>Hugo</a> & <a href=https://github.com/azmelanar/hugo-theme-pixyll target=_blank>Pixyll</a>
</small>
</div>
</div>
</footer>
<script src=../../js/highlight.pack.js></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>