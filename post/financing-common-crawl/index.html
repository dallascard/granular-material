<!doctype html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Financing Common Crawl - Granular Material</title>
<meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=author content><meta name=keywords content><link rel=canonical href=https://dallascard.github.io/granular-material/post/financing-common-crawl/><link rel=stylesheet type=text/css href=https://dallascard.github.io/granular-material//css/combined-min.css></head><body><div class=site-wrap><header class="site-header px2 px-responsive"><div class="mt2 wrap"><div class=measure><a href=https://dallascard.github.io/granular-material/ class=site-title>Granular Material</a><nav class="site-nav right"><a href=https://dallascard.github.io/granular-material//tags/>Tags</a>
<a href=https://dallascard.github.io/granular-material//about/>About</a></form></nav><div class=clearfix></div></div></div></header><div class="post p2 p-responsive wrap" role=main><div class=measure><div class="post-header mb2"><h1 class=py2>Financing Common Crawl</h1><span class=post-meta>Mar 31, 2024</span><br></div><article class=post-content><p>Mozilla recently published an excellent new <a href=https://foundation.mozilla.org/en/research/library/generative-ai-training-data/common-crawl/>report</a> out about Common Crawl, the non-profit whose web scrapes have played an important role in the development of numerous large language models (LLMs). Written by Stefan Baack and Mozilla Insights, the report is based on both public documents and new interviews with Common Crawl&rsquo;s current director and crawl engineer, and goes into some detail about the history of the organization, and how its data is being used.</p><p>As I have written <a href=https://www.sciencedirect.com/science/article/pii/S2666389924000746>elsewhere</a>, Common Crawl occupies a unique place in the LLM ecosystem. Although there are plenty of LLMs that have been trained using other data sources (including GPT-2, for which the creators directly scraped web pages linked from reddit), numerous landmark LLMs have used at least some Common Crawl data for pretraining.</p><p>The advantages of using Common Crawl for this purpose are obvious&mdash;it is huge and broad, comprising many terabytes of text content scraped from the web. As the name implies, this is a web <em>crawl</em>&mdash;starting from a few seeds, the crawlers follow links and scrape content as they go. This makes it very different from a web <em>archive</em>, such as the Internet Archive&mdash;which strives to maintain provenance, especially in the context of their <a href=https://web.archive.org/web/>Wayback Machine</a>, where it is possible to view and even interact with archived versions of web pages, similarly to how they would have appeared and functioned at many different points in the past.</p><p>As detailed in the Mozilla report, Common Crawl was created in 2007 as a non-profit to provide large scale data, scraped from the web, for purposes in research, business, and education. The data they collect is made freely available in snapshots, distributed via Amazon Web Services (AWS).</p><p>For a long time, it was not clear what this was all going to lead to. The first NLP paper I am aware of that used this data was a 2013 Machine Translation paper by Philipp Koehn, Chris Callison-Burch, Adam Lopez, and others, called &ldquo;<a href=https://aclanthology.org/P13-1135/>Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a>&rdquo;.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> To help locate this work in time, it seems like a large chunk of the parallel language text that those authors were able to extract from Common Crawl came from hotel booking sites.</p><p>Coincidentally enough, the first LLM trained on CommonCrawl data (that I am aware) of was <a href=https://arxiv.org/abs/1905.12616>Grover</a> (published in 2019), a model trained to generate (and defend against) fake news, developed by a team led by <a href=https://rowanzellers.com/>Rowan Zellers</a> and <a href=https://ariholtzman.com/>Ari Holtzman</a>, both of whom I shared an office with while at the University of Washington.<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> This was then followed by RoBERTa, T5, and GPT-3. In addition, archived snapshots of Common Crawl have been preserved in datasets such as <a href=https://arxiv.org/abs/1910.10683>C4</a> and <a href=https://arxiv.org/abs/2101.00027>The Pile</a>, which in turn have now been used to train many other models, such as LLaMa.</p><p>Going farther back in time, the Wayback Machine&rsquo;s <a href=https://web.archive.org/web/20080328041443/http://www.commoncrawl.org/>first snapshot</a> of the Common Crawl website is from March 28, 2008. At that point, it was pretty barebones, just providing the organization&rsquo;s mission at the time: &ldquo;CommonCrawl&rsquo;s mission is to build, maintain and make widely available a comprehensive crawl of the Internet for the purpose of enabling a new wave of innovation, education and research&rdquo;. There is also an <a href=https://web.archive.org/web/20080328041510/http://www.commoncrawl.org/faq.htm>FAQ</a>, aimed largely at people who might be noticing Common Crawl&rsquo;s bot scraping their websites, and wondering what it was.</p><p>By <a href=https://web.archive.org/web/20090419184554/http://www.commoncrawl.org/>April 19, 2009</a>, Common Crawl&rsquo;s website had added a beta rollout of a URL search feature. By <a href=https://web.archive.org/web/20111020192418/http://www.commoncrawl.org/>October 20, 2011</a>, they had a new website, and were finally providing actual data, distributed through a requester-pays API on AWS. The corresponding <a href=https://web.archive.org/web/20111020192300/http://www.commoncrawl.org/about/terms-of-use/>terms of use</a> suggested &ldquo;Don’t break the law or do anything illegal with our site or data&rdquo;, and listed &ldquo;Violate other people’s rights (IP, proprietary, etc.)&rdquo; and &ldquo;Invade other people’s privacy&rdquo; as examples of &ldquo;illegal stuff you can&rsquo;t do&rdquo;.</p><p>The new report from Mozilla does an especially good job of trying to correct a few key misconceptions. First among these is the idea that Common Crawl somehow represents &ldquo;<a href=https://dailynous.com/2020/07/30/philosophers-gpt-3/>the entire internet</a>&rdquo;. Far from this being the case, the web dumps provided by Common Crawl only index and distribute a small fraction of what is technically available.</p><p>Surprisingly, despite the title of their report (&ldquo;Training Data for the Price of a Sandwich&rdquo;), the one area that the Mozilla authors do not discuss in detail is how Common Crawl was actually financed. They mostly note in passing that, although Common Crawl is now seeking additional funding in the form of donations, it has been primarily funded since the beginning by its original founder, <a href=https://en.wikipedia.org/wiki/Gil_Elbaz>Gil Elbaz</a>; but that&rsquo;s about it.</p><p>What this misses is just how much money Elbaz has poured into the enterprise. Before founding Common Crawl, Elbaz was a co-founder of a start up called Applied Semantics, which was <a href=https://digiday.com/media/today-in-history-google-buys-applied-semantics/>acquired by Google</a> in 2003 for $102 million, along with the company&rsquo;s AdSense technology. In 2007, Elbaz left Google to start <a href="https://twitter.com/factual?lang=en">Factual</a>, (eventually aquired by Foursquare), with the intention of creating an open dataset to make sure the search giant didn&rsquo;t have a total monopoly on large scale internet data.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> In the same year, he also created the Common Crawl foundation.<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> As Common Crawl&rsquo;s current <a href=https://commoncrawl.org/mission>mission</a> states, it aims to provide &ldquo;high quality crawl data that was previously only available to large search engine corporations.&rdquo;</p><p>Using the <a href=https://projects.propublica.org/nonprofits/organizations/261635908>tax filings data</a> available from ProPublica, we can see that Elbaz has donated, through his foundation, around 4 million dollars to Common Crawl over the decade and a half from 2007 through 2022. An additional $155,000 was donated by Microsoft (in two chunks) about midway through that period. That, however, is the total sum of contributions that the non-profit has received up through 2022.</p><figure><a href=../../data/cc-funding.csv><img src=../../img/financing-common-crawl/cc_funding.png alt="Common Crawl Funding (2007-2022)."></a><figcaption><p>Common Crawl Funding (2007-2022).</p></figcaption></figure><p>On the one hand, four million dollars is a remarkably small amount of money, considering the impact that Common Crawl has had in recent years. It is becoming routine to hear about engineers working for OpenAI with high six- or even seven-figure salaries, and single LLM training runs now sometimes cost in the tens (or even hundreds) of millions of dollars, just to cover the cost of computation. From that perspective, $4 million seems like a drop in the bucket for a piece of infrastructure that has become such an important part of the LLM ecosystem.</p><p>On the other hand, $4 million is also a lot of money for an individual to invest in what might have seemed, especially early on, like something of a passion project. Much like <a href=https://en.wikipedia.org/wiki/Brewster_Kahle>Brewster Kahle</a>, creator of the Internet Archive, and another tech entrepreneur turned non-profit founder, Elbaz chose to take the earnings from the sale of his early ventures, and apply them to something somewhat singular and quirky.</p><p>Interestingly, while Common Crawl is the largest item on the balance sheet of the <a href=https://projects.propublica.org/nonprofits/organizations/206735811>Elbaz Family Foundation</a>, the foundation has also donated almost as much money to the X-Prize Foundation. Over the years it has also contributed generously to CalTech (Elbaz&rsquo;s alma mater), Claremont McKenna College, the UCLA law school, and many other recipients, including dozens of primarily liberal causes, such as the Natural Resources Defense Council, the Earth Island Institute, the Anti-Defamation League, and Iridescent Learning.</p><p>In some ways, Common Crawl has succeeded wildly in its mission, and for an incredibly economical price. There is no doubt that it has contributed massively (if indirectly) to &ldquo;a new wave of innovation, education and research&rdquo;, and it seems to (still) be almost the only openly available resource people think of for obtaining high quality crawl data at scale.</p><p>At the same time, it does seem like the primary direct beneficiaries of this work may end up being those companies, such as Google, Microsoft, OpenAI, and Meta, that have leveraged the existence of Common Crawl data in the process of inventing a new type of knowledge infrastructure&mdash;one which they seem increasingly likely to own and control, at least in its most widely used forms.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Interestingly, the paper&rsquo;s first author, Jason R. Smith, seems to have not published much else afterwards. <a href=https://dblp.org/pid/02/1964-6.html>DBLP</a> associates him (as Jason Smith) with <a href=https://arxiv.org/abs/1609.08144>another machine translation paper</a> from Google three years later. He is listed on <a href=https://www.cs.jhu.edu/~jason/Argo/>Jason Eisner&rsquo;s webpage</a>, (but not as Jason&rsquo;s dissertation advisor), and he does not appear on Philipp&rsquo;s or or Adam&rsquo;s web pages. My guess is that he either left or graduated and went to work for Google.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Rowan now works for OpenAI, and Ari is starting as an Assistant Professor at the University of Chicago this fall.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>&ldquo;Gil Elbaz On Google Acquiring His Company And Turning It Into A $15 Billion Business&rdquo;. <em>DealMakers: Entrepreneur | Startups | Venture Capital</em>. <a href=https://alejandrocremades.com/gil-elbaz-on-google-acquiring-his-company-and-turning-it-into-a-15-billion-business/>https://alejandrocremades.com/gil-elbaz-on-google-acquiring-his-company-and-turning-it-into-a-15-billion-business/</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p><a href=https://en.wikipedia.org/wiki/Gil_Elbaz>https://en.wikipedia.org/wiki/Gil_Elbaz</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></article><div style=text-align:center><span class=post-meta></span></div><br><p class=post-meta>Tags:&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/common-crawl/>common-crawl</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/knowledge-infrastructure/>knowledge-infrastructure</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/large-language-models/>large-language-models</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/archives/>archives</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/digital-preservation/>digital-preservation</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/data/>data</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/power/>power</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/bots/>bots</a></p><script src=https://utteranc.es/client.js repo=dallascard/granular-material issue-term=pathname label=comment theme=github-light crossorigin=anonymous async></script></div></div></div><footer class=footer><div class="p2 wrap"><div class="measure mt1 center"><nav class="social-icons icons"><a href=../../index.xml><img alt=RSS src=../../images/icons/rss.png></a>
<a href=https://twitter.com/dallascard><img src=../../images/icons/twitter.png alt=Twitter></a></nav><small>Copyright &#169; 2022<br>Powered by <a href=http://gohugo.io/ target=_blank>Hugo</a> & <a href=https://github.com/azmelanar/hugo-theme-pixyll target=_blank>Pixyll</a></small></div></div></footer><script src=../../js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-EL45TK68M9"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-EL45TK68M9")</script></body></html>