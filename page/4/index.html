<!doctype html><html><head><meta name=generator content="Hugo 0.144.2"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Granular Material - Granular Material</title>
<meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=author content><meta name=keywords content><link rel=canonical href=https://dallascard.github.io/granular-material/><link rel=stylesheet type=text/css href=https://dallascard.github.io/granular-material//css/combined-min.css></head><body><div class=site-wrap><header class="site-header px2 px-responsive"><div class="mt2 wrap"><div class=measure><a href=https://dallascard.github.io/granular-material/ class=site-title>Granular Material</a><nav class="site-nav right"><a href=https://dallascard.github.io/granular-material//tags/>Tags</a>
<a href=https://dallascard.github.io/granular-material//about/>About</a></nav><div class=clearfix></div></div></div></header><div class="post p2 p-responsive wrap" role=main><div class=measure><div class=home><div class=posts><div class=post><a class=post-link href=https://dallascard.github.io/granular-material/post/ai-dermatology-1/><p class="post-meta left">Feb 5, 2022</p><div class=clearfix></div><h3 class="h2 post-title">AI Dermatology: Part 1</h3><p class=post-summary><p>Midway through last year Google <a href=https://blog.google/technology/health/ai-dermatology-preview-io-2021/>announced</a> a new foray into the medical technology space, sharing that it was developing an &ldquo;AI-powered dermatology assist tool&rdquo;&mdash;a phone-based app that would allow users to take photos of skin lesions and retrieve information about relevant medical conditions from the web. <a href=https://www.skinvision.com/>Similar</a> <a href="https://play.google.com/store/apps/details?id=com.aidermatologist&amp;hl=en_US&amp;gl=US">apps</a> already exist, but it&rsquo;s fair to say that a comparable effort by Google is likely to have much more significant effects on how people interact with the medical system, their personal data, and even their own bodies.</p></p></a><p class=post-meta>Tags:&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/machine-learning/>machine-learning</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/dermatology/>dermatology</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/medicine/>medicine</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/data/>data</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/privacy/>privacy</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/foundation-models/>foundation-models</a></p></div><div class=post><a class=post-link href=https://dallascard.github.io/granular-material/post/stanford-vaccines/><p class="post-meta left">Dec 19, 2020</p><div class=clearfix></div><h3 class="h2 post-title">Vaccine Allocation at Stanford Hospital</h3><p class=post-summary>In a video that was widely shared last Friday, a representative from Stanford Medical Center spoke to residents protesting how the hospital chose to allocate its first shipment of COVID-19 vaccines. The hospital had around 5,000 initial doses to distribute (and expects to have tens of thousands more within the next few weeks), and came up with an allocation scheme in which only 7 of the approximately 1,300 residents were on the list. Many of these residents deal directly with patients who have COVID-19, whereas other more senior physicians, as well as other front line workers, such as nurses and food service employees, were given priority. In the video, the spokesperson explains that the algorithm they used to come up with an allocation scheme “clearly didn’t work”, to which protestors respond by shouting “Algorithms suck!” and “Fuck the algorithm!” &mldr;</p></a><p class=post-meta>Tags:&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/algorithmic-decision-making/>algorithmic-decision-making</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/medicine/>medicine</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/communication/>communication</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/covid/>covid</a></p></div><div class=post><a class=post-link href=https://dallascard.github.io/granular-material/post/science-critics/><p class="post-meta left">Oct 3, 2020</p><div class=clearfix></div><h3 class="h2 post-title">The case for professional critics in science</h3><p class=post-summary><p>In many areas of science, there is an increasingly urgent unmet need, a role that could be simultaneously fascinating, rewarding, and potential remunerative. It is a role that already exists in various forms, but which could be made into something much more potent, especially if forces converged to make it more prominent. I am talking, of course, about the professional science critic.</p><p>In the popular imagination, science operates something like a priesthood: scientists enter elite institutions as novices and emerge years later as full-fledged representatives of The Truth. Along the way, they are trained to be experts and professionals in their subject, to excel in action as well as in thought, and to sacrifice their worldly interests for the sake of their calling. The scholarly journals and peer review process are imagined to operate like an ecumenical council, guiding and filtering the thoughts of the broader community of the devout, and only allowing to pass what is deemed to be true and useful and good.</p></p></a><p class=post-meta>Tags:&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/science/>science</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/criticism/>criticism</a></p></div><div class=post><a class=post-link href=https://dallascard.github.io/granular-material/post/representational-power/><p class="post-meta left">Jul 20, 2019</p><div class=clearfix></div><h3 class="h2 post-title">Representational Power</h3><p class=post-summary><figure><a href=https://en.wikipedia.org/wiki/View_from_the_Window_at_Le_Gras><img src=../../img/representational-power//plate.jpeg alt="The original plate of “View from the Window at Le Gras”, a heliograph made by Nicéphore Niépce around 1827."></a><figcaption><p>The original plate of “View from the Window at Le Gras”, a heliograph made by Nicéphore Niépce around 1827.</p></figcaption></figure><p>Although it isn’t normally thought of in these terms, taking a photograph involves recording a four-dimensional block of space-time and projecting it down to a two-dimensional representation. With sufficiently sensitive material (and a fast enough shutter), one can produce images more or less instantaneously, but longer exposures reveal the inherent temporality of this process, showing us something that is clearly based on the world, yet quite different from our experience of it. Today, the ability to create images is so commonplace, of course, that we easily take it for granted, but early commentaries on photography reveal just how extraordinary it once was. Indeed, the history of photography provides both a compelling example of the power of representation, and a useful parallel to more recent forms of technological magic, especially that of machine learning.</p></p></a><p class=post-meta>Tags:&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/photography/>photography</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/representation/>representation</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/machine-learning/>machine-learning</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/power/>power</a></p></div><div class=post><a class=post-link href=https://dallascard.github.io/granular-material/post/face-recognition/><p class="post-meta left">Dec 17, 2018</p><div class=clearfix></div><h3 class="h2 post-title">On the Perils of Automated Face Recognition</h3><p class=post-summary><figure><a href=https://commons.wikimedia.org/wiki/File:Bertillon,_Alphonse,_fiche_anthropom%C3%A9trique_recto-verso.jpg><img src=../../img/face-recognition/bertillon.jpeg alt="Anthropometric data sheet (both sides) of Alphonse Bertillon (1853–1914)."></a><figcaption><p>Anthropometric data sheet (both sides) of Alphonse Bertillon (1853–1914).</p></figcaption></figure><p>For anyone who has been paying attention, it will not have gone unnoticed that the past year has seen a dramatic expansion in the use of face recognition technology, including at schools, border crossing, and interactions with the police. Most recently, <a href=https://news.delta.com/delta-unveils-first-biometric-terminal-us-atlanta-next-stop-detroit>Delta announced</a> that some passengers in Atlanta will be able to check in and go through security using only their face as identification. Most news coverage of this announcement emphasized the supposed convenience, efficiency, and technical novelty, while underplaying any potential hazards. In fact, however, the combination of widely available images, the ability to build on existing infrastructure, and a legal landscape that places very few restrictions on recording, means that face recognition represents a unique threat to privacy that should concern us greatly.</p></p></a><p class=post-meta>Tags:&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/privacy/>privacy</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/face-recognition/>face-recognition</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/machine-learning/>machine-learning</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/surveillance/>surveillance</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/biometrics/>biometrics</a></p></div><div class=post><a class=post-link href=https://dallascard.github.io/granular-material/post/interpretability/><p class="post-meta left">May 25, 2018</p><div class=clearfix></div><h3 class="h2 post-title">What everyone needs to know about interpretability in machine learning</h3><p class=post-summary>For anyone who’s been paying attention, it should be apparent that statistical machine learning systems are being widely deployed for automated decision making in all kinds of areas these days, including criminal justice, medicine, education, employment, policing, and so on. Particularly with the recently enacted GDPR&mdash;the new European regulation about data and privacy&mdash;there is growing interest in having systems that are interpretable, that is, we can make some sense of why they are making the prediction that they are making. To borrow an example from Been Kim, if a computer tells you that you need surgery, you’re probably going to ask for some sort of explanation.</p></a><p class=post-meta>Tags:&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/machine-learning/>machine-learning</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/interpretability/>interpretability</a></p></div><div class=post><a class=post-link href=https://dallascard.github.io/granular-material/post/privacy-in-context/><p class="post-meta left">May 14, 2018</p><div class=clearfix></div><h3 class="h2 post-title">Privacy in Context</h3><p class=post-summary><p>Although it was not the largest of its kind, or the most invasive, or even particularly surprising, the recent Cambridge Analytica scandal produced a surprisingly large amount of outrage and commentary. If nothing else, it was yet another reminder that we have gradually slipped into a regime where certain aspects of our privacy that could once be taken for granted are now long gone. Are people concerned? Is this something we should be worried about? What exactly are the harms that come from this sort of loss of privacy.</p></p></a><p class=post-meta>Tags:&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/privacy/>privacy</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/books/>books</a></p></div><div class=post><a class=post-link href=https://dallascard.github.io/granular-material/post/emergent-archives/><p class="post-meta left">Nov 27, 2017</p><div class=clearfix></div><h3 class="h2 post-title">Emergent Archives</h3><p class=post-summary><figure><img src=../../img/emergent-archives/archives.jpg></figure><p>It’s fascinating to think about how much we give to the internet, and sometimes, how much it gives back. In my mind, there is a fairly direct historical connection between self-archiving practices from a couple of decades ago (keeping a diary, making photo albums, writing letters, etc.), to all the more recent variations on this idea (status updates, tweets, photo feeds, etc.). The difference is arguably in how much of the background work is taken care of for us, and the fact that we now make so much of this information public or semi-public. While many have explored the implications of this for privacy, security, and surveillance, there is another aspect that gets less attention: the automatic creation of our own archives.</p></p></a><p class=post-meta>Tags:&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/archives/>archives</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/memory/>memory</a></p></div><div class=post><a class=post-link href=https://dallascard.github.io/granular-material/post/montage-as-archive/><p class="post-meta left">Nov 25, 2017</p><div class=clearfix></div><h3 class="h2 post-title">Montage as Archive</h3><p class=post-summary><p>I was recently reminded of Christian Marclay’s <a href=https://en.wikipedia.org/wiki/The_Clock_(2010_film)><em>The Clock</em></a>&mdash;a 24 hour film made up of thousands of short film clips, carefully edited together such that (when properly synchronized) the time depicted in each moment of film corresponds to the current time in the world. I’ve never seen it, and sadly only a few copies exist, but I think it’s a fascinating example of a kind of archival art.</p><p>When you hear the description, you might think of it as just a kind of a gimmick, one which would not reward actual viewing. However, based on the (bootlegged) clips that are <a href=https://kottke.org/13/06/about-an-hour-of-christian-marclays-the-clock>available online</a> and reports from various people who have seen it, it apparently achieves much more. To be clear, although many of the scenes involve a shot of some sort of time piece (thereby establishing the time in the source film), the composition is not so simple as just finding and assembling shots of clocks. Rather, we see a slice in time, sometimes with intercutting between films. As a result, common patterns quickly emerge: in the morning we get people waking up. In the evening, we see people eating dinner and attending parties, etc. This is interesting because what is shown in films bears some relation to what happens in the real world; even if it ends up being a distorted or idealized reflection, this certainly tells us something about how Hollywood depicts society, and by implication, something about society itself.</p></p></a><p class=post-meta>Tags:&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/archives/>archives</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/film/>film</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/history/>history</a></p></div><div class=post><a class=post-link href=https://dallascard.github.io/granular-material/post/reproducibility-in-art-and-science/><p class="post-meta left">Jun 5, 2017</p><div class=clearfix></div><h3 class="h2 post-title">Reproducibility in Art and Science</h3><p class=post-summary><p>Marking the beginning of a new <a href=http://rhizome.org/editorial/2017/may/30/rhizome-google-partnership/>partnership</a> between Rhizome and the Google Cultural Institute, the Rhizome blog recently published a <a href=http://rhizome.org/editorial/2017/may/30/preservation-by-accident-is-not-a-plan/>conversation</a> between Dragan Espenschied, Rhizome’s preservation director, and Vint Cerf, Google’s Chief Internet Evangelist. In addition to bringing together two people with among the coolest job titles ever, it got me thinking about some similarities between art and science when it comes to the issue of reproduction.</p><p>The main thrust of the conversation is about the difficulty of preserving internet art. In particular, works such as <a href=https://anthology.rhizome.org/the-web-stalker>The Web Stalker</a> were often made in a particular context, intended to be performances of a sort, and which depended on the existence of a certain technical infrastructure, including a particular operating system, a particular input device, an internet connection, and accompanying protocols. Part of the conversation relates to the fact that it’s actually very difficult to maintain or recreate every single thing that is required in order for a modern viewer to have the exact same experience as when it was first created. Nevertheless, Dragan argues, there is still value in preserving part of the experience.</p></p></a><p class=post-meta>Tags:&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/reproducibility/>reproducibility</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/art/>art</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/science/>science</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/digital-preservation/>digital-preservation</a>
,&nbsp;
<a href=https://dallascard.github.io/granular-material//tags/open-science/>open-science</a></p></div></div></div><div class=paginatin_area><div class="pagination clearfix mb1 mt4"><div class=left><a class=pagination-item href=../../page/3/>Newer</a></div><div class=right><a class=pagination-item href=../../page/5/>Older</a></div></div></div></div></div></div></div></div><footer class=footer><div class="p2 wrap"><div class="measure mt1 center"><nav class="social-icons icons"><a href=../../index.xml><img alt=RSS src=../../images/icons/rss.png></a>
<a href=https://twitter.com/dallascard><img src=../../images/icons/twitter.png alt=Twitter></a></nav><small>Copyright &#169; 2022<br>Powered by <a href=http://gohugo.io/ target=_blank>Hugo</a> & <a href=https://github.com/azmelanar/hugo-theme-pixyll target=_blank>Pixyll</a></small></div></div></footer><script src=../../js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-EL45TK68M9"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-EL45TK68M9")</script></body></html>